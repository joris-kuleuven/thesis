{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, matthews_corrcoef, f1_score, precision_score\n",
    "from numpy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of directories\n",
    "\n",
    "# this_dir = os.path.join(r\"C:\\Users\\yeska\\OneDrive - KU Leuven\\Documents\\master\", \"thesis\\ml\")\n",
    "# data_dir = os.path.join(this_dir, \"data\")\n",
    "ml_vars_dir = \"/Users/jorismachon/Documents/thesis/ML_data/vars\"\n",
    "# tc_v13_path = os.path.join(data_dir, \"tabulated_commits_v13_nov.json\")\n",
    "# tc_v14_path = os.path.join(data_dir, \"tabulated_commits_v14_nov.json\")\n",
    "tc_v14_path = \"/Users/jorismachon/Documents/thesis/tabulated_commits_v16.csv\"\n",
    "tc_v14_path = \"/Users/jorismachon/Documents/thesis/tabulated_commits_v14_nov.json\"\n",
    "jit_data = \"/home/deck/Documents/masterGT/mt/jit-vulnerability-prediction-master/scripts/ml_pipeline/data_for_analysis.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read this if this is your first time in this notebook\n",
    "\n",
    "This is not a neat notebook, might be a bit all over the place. Firstly, dont run the cells top down, its probably not gonna work. The first thing you want to run is the imports, and then the directories cell, you only need to change the \"this_dir\", change it to your own root directory!\n",
    "\n",
    "### Which cell to run?\n",
    "\n",
    "Go to section 3. Final function, it contains the cell the defines the main wrapper function. Run that cell defitions, go to the defintion of the children functions and run them as well, go to the defitions of the grandchildren functions and run them, and so on. The cells that defines these functions are a bit all over the place, so here is a quick breakdown of the ones that you need to run:\n",
    "\n",
    "- Section 1.2 contains def bool_cols_to_bin_cols(df) and def create_time_diff_col(df):\n",
    "- Section 1.3 contains def balancing(df, desired_ratio)\n",
    "- section 1.4 contains def feature_selection(X) and def calc_vif(X)\n",
    "- section 1.6 contains def data_preprocessing(source), this is the main wrapper function as explained in the beginning of section 1. Data pre-processing\n",
    "- section 2.1.1 contains def tune_hyperparameter(clf, name, cv), it's not used in the final function but you can run it if you want to test it\n",
    "- section 2.2.1 contains def train_cv_save_results(clf_name, cv, X_final, y_final, save_file_name, Xy_are_np=False) and def save_variables(file_name, variables)\n",
    "- section 2.3 contains def load_variables(file_name) and def load_show_metrics(file_name)\n",
    "- section 3 contains def train_ml_from_file(source, clf_name, save_file_name, folds=10, repeats=1), **this is the father of all the functions, this is the one that will be used to produce ml outputs from a source file**.\n",
    "\n",
    "### File namings for the results\n",
    "\n",
    "The final function train_ml_from_file(...) receives save_file_name, this is **only** the file_name! not the full path, also make sure that you add the extendion .pkl . The path is already set in the function. Also, please name it as {classifier_name}_k{number of folds}_r{number of repetitions}_{extra settings that you think are important}.pkl . For example, abc_k5_r5_v1.pkl means it uses ada boost classifier (and hence abc), 5 folds, 5 repertions, and version 1 (open up possibilities of other versions if necessary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data pre-processing\n",
    "\n",
    "1. Deal with null values\n",
    "2. Non-numerical values\n",
    "3. X and Y, plus balancing\n",
    "4. Feature selection with VIF\n",
    "5. Train, test split based on date\n",
    "6. **Main function for pre-processing**, this is the most important part if you want to run things (almost) right away. go to this section and run the cell that defines data_preprocessing(...). From that cell, ensure that you run the cells that defines the functions used in data_preprocessing(...), you can simply ctrl+leftclick the function (in vscode) to go the right function definition cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick data flattening\n",
    "\n",
    "tabulated_commits_v13_nov.json has a dictionary in each entry, needed to be flattened. Result is tabulated_commits_v14_nov.json.These cells do not need to be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(tc_v13_path, 'r') as f:\n",
    "    tc_v13 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_json(json_data, parent_key='', sep='_'):\n",
    "    \"\"\"\n",
    "    Flatten nested JSON structure\n",
    "    \"\"\"\n",
    "    items = {}\n",
    "    for k, v in json_data.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.update(flatten_json(v, new_key, sep=sep))\n",
    "        else:\n",
    "            items[new_key] = v\n",
    "    return items\n",
    "\n",
    "flattened_data = [flatten_json(item) for item in tc_v13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tc_v14_path, 'w') as f:\n",
    "    json.dump(flattened_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Null values dealing (and init df)\n",
    "Early pandas stuff, just to get to know the data a bit. There's also a line that replaces the null values with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tc_v14_df = pd.read_json(tc_v14_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_v14_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are actually quite a lot of null values\n",
    "\n",
    "null_author_x_count = tc_v14_df['author_x'].isnull().sum()\n",
    "not_null_author_x_count = tc_v14_df['author_x'].notnull().sum()\n",
    "\n",
    "php_files_not_exist = tc_v14_df['oop_php_files_exist'].isnull().sum() + len(tc_v14_df[tc_v14_df[\"oop_php_files_exist\"]==0])\n",
    "php_files_exist = len(tc_v14_df[tc_v14_df[\"oop_php_files_exist\"]==1])\n",
    "\n",
    "print(\"total data is\", len(tc_v14_df))\n",
    "print(\"No author count\", null_author_x_count)\n",
    "print(\"Yes author count\", not_null_author_x_count)\n",
    "print(\"php files not exist (or not extracted) count\", php_files_not_exist)\n",
    "print(\"php files exist count\", php_files_exist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there's a lot of null values, we still don't know what to do with it, so we're going to replace it with -1 for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_v14_df_filled = tc_v14_df.copy()\n",
    "tc_v14_df_filled.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Non-numerical processings\n",
    "\n",
    "This is dropping some columns and changing some bool columns to binary. We also make the author creation date a time difference to the commit date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN FUNCTIONS\n",
    "def bool_cols_to_bin_cols(df):\n",
    "    bin_cols = [\"fix\", \"is_vulnerable\", \"neutral\", \"new_author\", \"oop_php_files_exist\"]\n",
    "    # Change binaries to 0 and 1\n",
    "    for col in bin_cols:\n",
    "        df[col] = df[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nonnumeric_cols(df):\n",
    "    # Identify numeric columns\n",
    "    numeric_columns = df.select_dtypes(include='number').columns\n",
    "\n",
    "    # List out non-numeric columns\n",
    "    non_numeric_columns = df.columns.difference(numeric_columns)\n",
    "\n",
    "    # Print non-numeric columns\n",
    "    print(\"Non-numeric columns:\")\n",
    "    for column in non_numeric_columns:\n",
    "        print(column)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_cols = [\"fix\", \"is_vulnerable\", \"neutral\", \"new_author\", \"oop_php_files_exist\"]\n",
    "# Change binaries to 0 and 1\n",
    "for col in bin_cols:\n",
    "    tc_v14_df_filled[col] = tc_v14_df_filled[col].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the function run below is to check if there's any nonnumeric cols left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nonnumeric_cols()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the better version for the timezone, use it for next iterations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TIMEZONE\n",
    "tc_v14_df_filled['author_info_created_at'].replace(-1, np.nan, inplace=True)\n",
    "tc_v14_df_filled['author_date'] = pd.to_datetime(tc_v14_df_filled['author_date'], unit='ms')\n",
    "tc_v14_df_filled['author_timezone'] = pd.to_timedelta(tc_v14_df_filled['author_timezone'], unit='s')\n",
    "# Combine author_date and author_timezone to create a new column representing the actual date and time\n",
    "tc_v14_df_filled['commit_time'] = tc_v14_df_filled['author_date'] + tc_v14_df_filled['author_timezone']\n",
    "\n",
    "print(\"length df: \", len(tc_v14_df_filled))\n",
    "print(\"number of unique commit SHAs: \", len(tc_v14_df_filled['commit_sha'].unique()))\n",
    "\n",
    "# columns_to_drop = df.filter(regex='^BoW_').columns\n",
    "# df_final = df.drop(columns=columns_to_drop)\n",
    "\n",
    "tc_v14_df_filled['author_info_created_at'] = pd.to_datetime(tc_v14_df_filled['author_info_created_at'], errors='coerce', utc=True)\n",
    "tc_v14_df_filled['commit_time'] = pd.to_datetime(tc_v14_df_filled['commit_time'], errors='coerce', utc=True)\n",
    "\n",
    "tc_v14_df_filled.loc[tc_v14_df_filled['author_x'] != None, 'time_difference'] = (tc_v14_df_filled.loc[tc_v14_df_filled['author_x'] != None, 'commit_time'] - tc_v14_df_filled.loc[tc_v14_df_filled['author_x'] != None, 'author_info_created_at']).dt.days\n",
    "tc_v14_df_filled['time_difference'].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_diff_col(df):\n",
    "    # TIMEZONE\n",
    "    df['author_info_created_at'].replace(-1, np.nan, inplace=True)\n",
    "    df['author_date'] = pd.to_datetime(df['author_date'], unit='ms')\n",
    "    df['author_timezone'] = pd.to_timedelta(df['author_timezone'], unit='s')\n",
    "    # Combine author_date and author_timezone to create a new column representing the actual date and time\n",
    "    df['commit_time'] = df['author_date'] + df['author_timezone']    \n",
    "\n",
    "    # columns_to_drop = df.filter(regex='^BoW_').columns\n",
    "    # df_final = df.drop(columns=columns_to_drop)\n",
    "\n",
    "    df['author_info_created_at'] = pd.to_datetime(df['author_info_created_at'], errors='coerce', utc=True)\n",
    "    df['commit_time'] = pd.to_datetime(df['commit_time'], errors='coerce', utc=True)\n",
    "\n",
    "    df.loc[df['author_x'] != None, 'time_difference'] = (df.loc[df['author_x'] != None, 'commit_time'] - df.loc[df['author_x'] != None, 'author_info_created_at']).dt.days\n",
    "    df['time_difference'].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_v14_df_filled = tc_v14_df_filled.drop(columns=[\"appname\", \"author_info_username\", \"author_x\", \"author_y\", \"commit_sha\", \"repo\", \"author_info_created_at\", \"author_date\", \"author_timezone\"])\n",
    "print(tc_v14_df_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_nonnumeric_cols()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "commit time is kept because initially we want to split the data by date, but since we're using repeated fold cross-validation, it's gonna be kinda complicated to do that. This commit_time feature will be removed later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 X and Y, plus balancing\n",
    "\n",
    "balancing here is undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tc_v14_df_filled['is_vulnerable']\n",
    "X = tc_v14_df_filled.drop(columns=['is_vulnerable'])\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#undersampling\n",
    "desired_ratio = 1 \n",
    "rus = RandomUnderSampler(sampling_strategy=desired_ratio, random_state=42)\n",
    "X_balanced, y_balanced = rus.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_resampled shape:\", X_balanced.shape)\n",
    "print(\"y_resampled shape:\", y_balanced.shape)\n",
    "\n",
    "print(\"vuln count\", len(y_balanced[y_balanced==1]))\n",
    "print(\"non vuln count\", len(y_balanced[y_balanced==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancing(df, desired_ratio):    \n",
    "    y = df['is_vulnerable']\n",
    "    X = df.drop(columns=['is_vulnerable'])\n",
    "    rus = RandomUnderSampler(sampling_strategy=desired_ratio, random_state=42)\n",
    "    X_balanced, y_balanced = rus.fit_resample(X, y)\n",
    "    return X_balanced, y_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Feature selection with VIF\n",
    "\n",
    "VIF removes features that have collinearity, used in palomba jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_vif(X):\n",
    "\n",
    "    # Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    return(vif)\n",
    "\n",
    "def feature_selection(X):\n",
    "    vif1 = calc_vif(X)\n",
    "    a=vif1.VIF.max()\n",
    "    while a > 5:\n",
    "        maximum_a = vif1.loc[vif1['VIF'] == vif1['VIF'].max()]\n",
    "        vif1 = vif1.loc[vif1['variables'] != maximum_a.iloc[0,0]]\n",
    "        vif1 = calc_vif(X[vif1.variables.tolist()])\n",
    "        a = vif1.VIF.max()\n",
    "        # print(a)\n",
    "\n",
    "    X = X[vif1.variables.tolist()]\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colX = [c for c in feature_selection(X_balanced.drop(columns=['commit_time']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colX)\n",
    "print(len(colX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colX.append('commit_time')\n",
    "X_vif = X_balanced[colX]\n",
    "print(X_vif.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Train test split with date\n",
    "\n",
    "this is ended up not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_based_on_time(X, y, ratio):\n",
    "    # Sort the DataFrame by commit_time\n",
    "    data = pd.concat([X, y], axis=1)\n",
    "    data = data.sort_values(by='commit_time')\n",
    "\n",
    "    # Calculate the index for splitting\n",
    "    split_index = int(len(data) * ratio)\n",
    "\n",
    "    # Split the DataFrame into training and test sets based on the calculated index\n",
    "    train_data = data.iloc[:split_index]\n",
    "    test_data = data.iloc[split_index:]\n",
    "\n",
    "    # Separate X and y for training and test sets\n",
    "    X_train = train_data.drop(columns=['is_vulnerable'])  # Assuming 'commit_time' is not a feature\n",
    "    y_train = train_data['is_vulnerable']\n",
    "    X_test = test_data.drop(columns=['is_vulnerable'])    # Assuming 'commit_time' is not a feature\n",
    "    y_test = test_data['is_vulnerable']\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def train_val_test_split(X, y, ratio):\n",
    "    X_train, y_train, X_val_test, y_val_test = split_based_on_time(X, y, ratio)\n",
    "    X_val, y_val, X_test, y_test = split_based_on_time(X_val_test, y_val_test, 0.5)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X_vif, y_balanced, 0.6)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=[\"commit_time\"], inplace=True)\n",
    "X_val.drop(columns=[\"commit_time\"], inplace=True)\n",
    "X_test.drop(columns=[\"commit_time\"], inplace=True)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_train[y_train==1]))\n",
    "print(len(y_test[y_test==1]))\n",
    "print(len(y_val[y_val==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Main pre-processing function\n",
    "\n",
    "this is the main function that is used in the final function for training machine learning. Its a wrapper for everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(source):\n",
    "    print(\"Pre-processing data...\")\n",
    "    \n",
    "    \n",
    "    # Check if the file ends with .csv or .json\n",
    "    if source.endswith('.csv'):\n",
    "        # Read CSV file\n",
    "        tc_df = pd.read_csv(source)\n",
    "    elif source.endswith('.json'):\n",
    "        # Read JSON file\n",
    "        tc_df = pd.read_json(source)\n",
    "    else:\n",
    "        # If the file extension is neither .csv nor .json, raise an error\n",
    "        raise ValueError(\"Unsupported file format. Please provide a CSV or JSON file.\")\n",
    "    print(\"Non-numerical processing\")\n",
    "    tc_df.fillna(-1, inplace=True) #Replace all null values with -1\n",
    "    \n",
    "    bool_cols_to_bin_cols(tc_df) #boolean columns to binary\n",
    "    \n",
    "    create_time_diff_col(tc_df) #create a time difference col that calculates the time diff between author creation and commit time\n",
    "    \n",
    "    tc_df = tc_df.drop(columns=[\"appname\", #drop unused columns\n",
    "                                                      \"author_info_username\", \n",
    "                                                      \"author_x\", \"author_y\", \n",
    "                                                      \"commit_sha\", \"repo\", \n",
    "                                                      \"author_info_created_at\", \n",
    "                                                      \"author_date\", \n",
    "                                                      \"author_timezone\"])\n",
    "    print(\"Balancing...\")\n",
    "    X, y = balancing(tc_df, 1)\n",
    "    print(\"Feature selection with VIF...\")\n",
    "    colX = [c for c in feature_selection(X.drop(columns=['commit_time']))]\n",
    "    X = X[colX]\n",
    "    print(\"Pre-processing done!\\n\")\n",
    "    return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the function\n",
    "X_final_test, y_final_test = data_preprocessing(tc_v14_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_final_test.shape, y_final_test.shape)\n",
    "print(X_final_test.values[1427])\n",
    "print(X_final_np[1427])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter tuning, training, and show metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Hyperparameter tuning\n",
    "\n",
    "the column commit_time is dropped here (for no good reason, but its not a big deal lol). RandomSearchCV is used for the tuning as it is used in the Palomba paper. Again simply go to the main function for hyperparameter tuning and run it if you want a (semi) plug and play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = X_vif.drop(columns = [\"commit_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_final.shape, y_balanced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifiers = [(AdaBoostClassifier(n_estimators=100, random_state=0), \"AdaBoostClassifier\")]\n",
    "abc = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\n",
    "n_estimators = np.arange(100, 2000, step=100)\n",
    "learning_rate = [0.001, 0.01, 0.1, 0.2, 0.5]\n",
    "\n",
    "param_space = {\n",
    "    \"n_estimators\": n_estimators,\n",
    "    \"learning_rate\": learning_rate\n",
    "}\n",
    "random_search = RandomizedSearchCV(abc, param_space, n_iter = 10, cv = cv, scoring='f1', n_jobs=-1, random_state=0, verbose=1)\n",
    "\n",
    "search = random_search.fit(X_final, y_balanced)\n",
    "\n",
    "best_param = search.best_params_\n",
    "best_score = search.best_score_\n",
    "print(best_param)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final_np = X_final.values\n",
    "y_balanced_np = y_balanced.values\n",
    "print(X_final_np.shape, y_balanced_np.shape)\n",
    "print(\"Xfinal is\", X_final_np)\n",
    "print(\"yfinal is\", y_balanced_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Hyperparameter tuning: Wrapper function\n",
    "run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping function for hyperparam tuning\n",
    "def tune_hyperparameter(clf, name, cv):    \n",
    "    if name == \"AdaBoostClassifier\":\n",
    "        n_estimators = np.arange(100, 2000, step=100)\n",
    "        learning_rate = [0.001, 0.01, 0.1, 0.2, 0.5]\n",
    "\n",
    "        param_space = {\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"learning_rate\": learning_rate\n",
    "        }\n",
    "    else:\n",
    "        raise Exception(\"Hyper parameter tuning param space not set for classifier\", name)\n",
    "    \n",
    "    random_search = RandomizedSearchCV(clf, param_space, n_iter = 10, cv = cv, scoring='f1', n_jobs=-1, random_state=0, verbose=1)\n",
    "\n",
    "    search = random_search.fit(X_final, y_balanced)\n",
    "\n",
    "    best_param = search.best_params_\n",
    "    best_score = search.best_score_\n",
    "    print(\"the best parameters are\", best_param)\n",
    "    print(\"with (the best) score being\", best_score)\n",
    "    return best_param, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the hyperparam tuning\n",
    "abc = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\n",
    "best_param_test, best_score_test = tune_hyperparameter(abc, \"AdaBoostClassifier\", cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training the model\n",
    "\n",
    "the first cell is the first script that was tested to run. The cells after are the wrapper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_tuned = AdaBoostClassifier(n_estimators=1900, learning_rate=0.1, random_state=0)\n",
    "splits_indices = cv.split(X_final_np, y_balanced_np)\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "tprs = []\n",
    "aucs = []\n",
    "\n",
    "N, P = X_final_np.shape\n",
    "\n",
    "# Aggregate the importances over folds here:\n",
    "importances_random = np.zeros(P)\n",
    "\n",
    "# Loop over crossvalidation folds:\n",
    "scores = []  # Collect accuracies here\n",
    "\n",
    "TP = []\n",
    "FP = []\n",
    "TN = []\n",
    "FN = []\n",
    "tnList = []\n",
    "fpList = []\n",
    "fnList = []\n",
    "tpList = []\n",
    "precisionList = []\n",
    "f1List = []\n",
    "mccList = []\n",
    "\n",
    "i = 1\n",
    "count = 0\n",
    "# for train, test in cv.split(X, y):\n",
    "train_splits = []\n",
    "test_splits = []\n",
    "train_anomaly_percentage = []\n",
    "test_anomaly_percentage = []\n",
    "train_anomaly_absolute = []\n",
    "test_anomaly_absolute = []\n",
    "counterfold = 1\n",
    "for train, test in splits_indices:\n",
    "    print('Fold %s of 10' %counterfold)\n",
    "    counterfold+=1\n",
    "    train_splits.append(train)\n",
    "    test_splits.append(test)\n",
    "    count += 1\n",
    "\n",
    "    X_train = X_final_np[train]\n",
    "    y_train = y_balanced_np[train]\n",
    "    X_test = X_final_np[test]\n",
    "    y_test = y_balanced_np[test]\n",
    "\n",
    "    a, b = np.unique(y_train, return_counts=True)[1]\n",
    "    train_anomaly_percentage.append(b / (a + b))\n",
    "    train_anomaly_absolute.append(b)\n",
    "    c, d = np.unique(y_test, return_counts=True)[1]\n",
    "    test_anomaly_percentage.append(d / (c + d))\n",
    "    test_anomaly_absolute.append(d)\n",
    "\n",
    "    abc_tuned.fit(X_train, y_train)\n",
    "\n",
    "    # Predict for validation data_raw:\n",
    "\n",
    "    \n",
    "    probas_ = abc_tuned.predict_proba(X_test)\n",
    "    y_pred = abc_tuned.predict(X_test)\n",
    "\n",
    "    # Compute ROC curve and area under the curve\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1], pos_label=1)\n",
    "\n",
    "    tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "    tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucs.append(roc_auc)\n",
    "\n",
    "    # calculate confusion matrix, precision, f1 and Matthews Correlation Coefficient\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    TN.append(tn)\n",
    "    TP.append(tp)\n",
    "    FN.append(fn)\n",
    "    FP.append(fp)\n",
    "\n",
    "    tnList.append(tn / (tn + fp))\n",
    "    tpList.append(tp / (fn + tp))\n",
    "    fpList.append(fp / (tn + fp))\n",
    "    fnList.append(fn / (fn + tp))\n",
    "\n",
    "    precisionList.append(precision)\n",
    "    f1List.append(f1)\n",
    "    mccList.append(mcc)\n",
    "\n",
    "    i += 1\n",
    "    print(\"\\n\")\n",
    "    \n",
    "print(\"confusion matrix\")\n",
    "tnList = 100 * np.array(tnList)\n",
    "tpList = 100 * np.array(tpList)\n",
    "fnList = 100 * np.array(fnList)\n",
    "fpList = 100 * np.array(fpList)\n",
    "precisionList = 100 * np.array(precisionList)\n",
    "f1List = 100 * np.array(f1List)\n",
    "mccList = 100 * np.array(mccList)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "# mean_auc = auc(mean_fpr, mean_tpr)\n",
    "mean_auc = np.mean(aucs)\n",
    "std_auc = np.std(aucs)\n",
    "auc_meanpercent = 100 * mean_auc\n",
    "auc_stdpercent = 100 * std_auc\n",
    "\n",
    "variables_to_save = {\n",
    "    'tprs': tprs,\n",
    "    'aucs': aucs,\n",
    "    'N': N,\n",
    "    'P': P,\n",
    "    'importances_random': importances_random,\n",
    "    'scores': scores,\n",
    "    'TP': TP,\n",
    "    'FP': FP,\n",
    "    'TN': TN,\n",
    "    'FN': FN,\n",
    "    'tnList': tnList,\n",
    "    'fpList': fpList,\n",
    "    'fnList': fnList,\n",
    "    'tpList': tpList,\n",
    "    'precisionList': precisionList,\n",
    "    'f1List': f1List,\n",
    "    'mccList': mccList,\n",
    "    'train_splits': train_splits,\n",
    "    'test_splits': test_splits,\n",
    "    'train_anomaly_percentage': train_anomaly_percentage,\n",
    "    'test_anomaly_percentage': test_anomaly_percentage,\n",
    "    'train_anomaly_absolute': train_anomaly_absolute,\n",
    "    'test_anomaly_absolute': test_anomaly_absolute\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Training the model: wrapper functions\n",
    "\n",
    "the main function is train_cv_save_results(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_variables(file_name, variables):\n",
    "    file_path = os.path.join(ml_vars_dir, file_name)\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pickle.dump(variables, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cv_save_results(clf_name, cv, X_final, y_final, save_file_name, Xy_are_np=False):\n",
    "    if not Xy_are_np:    \n",
    "        X_final_np = X_final.values\n",
    "        y_final_np = y_final.values\n",
    "    else:\n",
    "        X_final_np = X_final\n",
    "        y_final_np = y_final\n",
    "    if clf_name == \"AdaBoostClassifier\":\n",
    "        clf = AdaBoostClassifier(n_estimators=1900, learning_rate=0.1, random_state=0)\n",
    "    else:\n",
    "        raise Exception(\"Clf not set for clf name\", clf_name)\n",
    "    splits_indices = cv.split(X_final_np, y_final_np)\n",
    "\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "\n",
    "    N, P = X_final_np.shape\n",
    "\n",
    "    # Aggregate the importances over folds here:\n",
    "    importances_random = np.zeros(P)\n",
    "\n",
    "    # Loop over crossvalidation folds:\n",
    "    scores = []  # Collect accuracies here\n",
    "\n",
    "    TP = []\n",
    "    FP = []\n",
    "    TN = []\n",
    "    FN = []\n",
    "    tnList = []\n",
    "    fpList = []\n",
    "    fnList = []\n",
    "    tpList = []\n",
    "    precisionList = []\n",
    "    f1List = []\n",
    "    mccList = []\n",
    "\n",
    "    i = 1\n",
    "    count = 0\n",
    "    # for train, test in cv.split(X, y):\n",
    "    train_splits = []\n",
    "    test_splits = []\n",
    "    train_anomaly_percentage = []\n",
    "    test_anomaly_percentage = []\n",
    "    train_anomaly_absolute = []\n",
    "    test_anomaly_absolute = []\n",
    "    counterfold = 1\n",
    "    for train, test in splits_indices:\n",
    "        print(\"Fold-repetition\", counterfold)\n",
    "        counterfold+=1\n",
    "        train_splits.append(train)\n",
    "        test_splits.append(test)\n",
    "        count += 1\n",
    "\n",
    "        X_train = X_final_np[train]\n",
    "        y_train = y_final_np[train]\n",
    "        X_test = X_final_np[test]\n",
    "        y_test = y_final_np[test]\n",
    "\n",
    "        a, b = np.unique(y_train, return_counts=True)[1]\n",
    "        train_anomaly_percentage.append(b / (a + b))\n",
    "        train_anomaly_absolute.append(b)\n",
    "        c, d = np.unique(y_test, return_counts=True)[1]\n",
    "        test_anomaly_percentage.append(d / (c + d))\n",
    "        test_anomaly_absolute.append(d)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict for validation data_raw:\n",
    "\n",
    "        \n",
    "        probas_ = clf.predict_proba(X_test)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Compute ROC curve and area under the curve\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1], pos_label=1)\n",
    "\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "\n",
    "        # calculate confusion matrix, precision, f1 and Matthews Correlation Coefficient\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        TN.append(tn)\n",
    "        TP.append(tp)\n",
    "        FN.append(fn)\n",
    "        FP.append(fp)\n",
    "\n",
    "        tnList.append(tn / (tn + fp))\n",
    "        tpList.append(tp / (fn + tp))\n",
    "        fpList.append(fp / (tn + fp))\n",
    "        fnList.append(fn / (fn + tp))\n",
    "\n",
    "        precisionList.append(precision)\n",
    "        f1List.append(f1)\n",
    "        mccList.append(mcc)\n",
    "\n",
    "        i += 1\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    tnList = 100 * np.array(tnList)\n",
    "    tpList = 100 * np.array(tpList)\n",
    "    fnList = 100 * np.array(fnList)\n",
    "    fpList = 100 * np.array(fpList)\n",
    "    precisionList = 100 * np.array(precisionList)\n",
    "    f1List = 100 * np.array(f1List)\n",
    "    mccList = 100 * np.array(mccList)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    # mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    mean_auc = np.mean(aucs)\n",
    "    std_auc = np.std(aucs)\n",
    "    auc_meanpercent = 100 * mean_auc\n",
    "    auc_stdpercent = 100 * std_auc\n",
    "\n",
    "    variables_to_save = {\n",
    "        'tprs': tprs,\n",
    "        'aucs': aucs,\n",
    "        'N': N,\n",
    "        'P': P,\n",
    "        'importances_random': importances_random,\n",
    "        'scores': scores,\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'TN': TN,\n",
    "        'FN': FN,\n",
    "        'tnList': tnList,\n",
    "        'fpList': fpList,\n",
    "        'fnList': fnList,\n",
    "        'tpList': tpList,\n",
    "        'precisionList': precisionList,\n",
    "        'f1List': f1List,\n",
    "        'mccList': mccList,\n",
    "        'train_splits': train_splits,\n",
    "        'test_splits': test_splits,\n",
    "        'train_anomaly_percentage': train_anomaly_percentage,\n",
    "        'test_anomaly_percentage': test_anomaly_percentage,\n",
    "        'train_anomaly_absolute': train_anomaly_absolute,\n",
    "        'test_anomaly_absolute': test_anomaly_absolute,\n",
    "        'auc_meanpercent': auc_meanpercent,\n",
    "        'auc_stdpercent' : auc_stdpercent\n",
    "    }\n",
    "    save_variables(save_file_name, variables_to_save)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test train wrapper function\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\n",
    "train_cv_save_results(\"AdaBoostClassifier\", cv, X_final_test, y_final_test, \"abc_k10_r1_v2.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Load metrics and show\n",
    "\n",
    "The main function is load_show_metrics(...). Run that and also load_variables(...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_variables(file_name):\n",
    "    file_path = os.path.join(ml_vars_dir, file_name)\n",
    "    with open(file_path, 'rb') as handle:\n",
    "        loaded_variables = pickle.load(handle)    \n",
    "    return loaded_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_show_metrics(file_name):\n",
    "    print(\"Showing metrics for file\", file_name)\n",
    "    # Load variables from the file\n",
    "    loaded_variables = load_variables(file_name)\n",
    "    # Return each variable separately\n",
    "    tprs = loaded_variables['tprs']\n",
    "    aucs = loaded_variables['aucs']\n",
    "    N = loaded_variables['N']\n",
    "    P = loaded_variables['P']\n",
    "    importances_random = loaded_variables['importances_random']\n",
    "    scores = loaded_variables['scores']\n",
    "    TP = loaded_variables['TP']\n",
    "    FP = loaded_variables['FP']\n",
    "    TN = loaded_variables['TN']\n",
    "    FN = loaded_variables['FN']\n",
    "    tnList = loaded_variables['tnList']\n",
    "    fpList = loaded_variables['fpList']\n",
    "    fnList = loaded_variables['fnList']\n",
    "    tpList = loaded_variables['tpList']\n",
    "    precisionList = loaded_variables['precisionList']\n",
    "    f1List = loaded_variables['f1List']\n",
    "    mccList = loaded_variables['mccList']\n",
    "    train_splits = loaded_variables['train_splits']\n",
    "    test_splits = loaded_variables['test_splits']\n",
    "    train_anomaly_percentage = loaded_variables['train_anomaly_percentage']\n",
    "    test_anomaly_percentage = loaded_variables['test_anomaly_percentage']\n",
    "    train_anomaly_absolute = loaded_variables['train_anomaly_absolute']\n",
    "    test_anomaly_absolute = loaded_variables['test_anomaly_absolute']\n",
    "    \n",
    "    mean_auc = np.mean(aucs)\n",
    "    std_auc = np.std(aucs)\n",
    "    auc_meanpercent = 100 * mean_auc\n",
    "    auc_stdpercent = 100 * std_auc\n",
    "    \n",
    "    \"\"\"Show metrics\"\"\"\n",
    "    \n",
    "    plt.clf()  # Clear the current figure\n",
    "    \n",
    "    print(\"TN: %.02f %% ± %.02f %% - FN: %.02f %% ± %.02f %%\" % (np.mean(tnList),\n",
    "                                                                    np.std(tnList),\n",
    "                                                                    np.mean(fnList),\n",
    "                                                                    np.std(fnList)))\n",
    "    print(\"FP: %.02f %% ± %.02f %% - TP: %.02f %% ± %.02f %%\" % (np.mean(fpList),\n",
    "                                                                    np.std(fpList),\n",
    "                                                                    np.mean(tpList),\n",
    "                                                                    np.std(tpList)))\n",
    "\n",
    "    print(\n",
    "        \"Precision: %.02f %% ± %.02f %% - F1: %.02f %% ± %.02f %% - MCC: %.02f %% ± %.02f %%\" % (np.mean(precisionList),\n",
    "                                                                                                    np.std(precisionList),\n",
    "                                                                                                    np.mean(f1List),\n",
    "                                                                                                    np.std(f1List),\n",
    "                                                                                                    np.mean(mccList),\n",
    "                                                                                                    np.std(mccList)))\n",
    "\n",
    "    print(\"AUC: %.02f %% ± %.02f %%\" % (auc_meanpercent, auc_stdpercent))\n",
    "    # plt.figure(1)  # Create a new figure\n",
    "    # plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "    #             label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "    #             lw=2, alpha=.8)\n",
    "\n",
    "    # std_tpr = np.std(tprs, axis=0)\n",
    "    # tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    # tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    # plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "    #                     label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    # xlim = [-0.05, 1.05]\n",
    "    # ylim = [-0.05, 1.05]\n",
    "    # plt.xlim(xlim)\n",
    "    # plt.ylim(ylim)\n",
    "    # plt.xlabel('False Positive Rate')\n",
    "    # plt.ylabel('True Positive Rate')\n",
    "    # plt.legend(loc=\"lower right\")\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test show load metrics wrapper func\n",
    "load_show_metrics(\"abc_k10_r1_v1.pkl\")\n",
    "load_show_metrics(\"abc_k5_r5_v1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Show metrics\"\"\"\n",
    "\n",
    "print(\"TN: %.02f %% ± %.02f %% - FN: %.02f %% ± %.02f %%\" % (np.mean(tnList),\n",
    "                                                                np.std(tnList),\n",
    "                                                                np.mean(fnList),\n",
    "                                                                np.std(fnList)))\n",
    "print(\"FP: %.02f %% ± %.02f %% - TP: %.02f %% ± %.02f %%\" % (np.mean(fpList),\n",
    "                                                                np.std(fpList),\n",
    "                                                                np.mean(tpList),\n",
    "                                                                np.std(tpList)))\n",
    "\n",
    "print(\n",
    "    \"Precision: %.02f %% ± %.02f %% - F1: %.02f %% ± %.02f %% - MCC: %.02f %% ± %.02f %%\" % (np.mean(precisionList),\n",
    "                                                                                                np.std(precisionList),\n",
    "                                                                                                np.mean(f1List),\n",
    "                                                                                                np.std(f1List),\n",
    "                                                                                                np.mean(mccList),\n",
    "                                                                                                np.std(mccList)))\n",
    "\n",
    "print(\"AUC: %.02f %% ± %.02f %%\" % (auc_meanpercent, auc_stdpercent))\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(mean_fpr, mean_tpr, color='b',\n",
    "            label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "            lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                    label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "xlim = [-0.05, 1.05]\n",
    "ylim = [-0.05, 1.05]\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Final function\n",
    "\n",
    "This is the main dish, its the ultra main function that wraps everything. It takes the file, what kind of classifier you want (right now it only supports adaboost), outputfile name, and some cv settings. Run the \"def train_ml_from_file(...)\" cell, and go to the child functions and run their definition cells as well, and run the children functions of those children functions and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ml_from_file(source, clf_name, save_file_name, folds=10, repeats=1):\n",
    "    X_final, y_final = data_preprocessing(source)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=folds, n_repeats=repeats, random_state=1)\n",
    "    train_cv_save_results(clf_name, cv, X_final, y_final, save_file_name)\n",
    "    load_show_metrics(save_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ml_from_file(tc_v14_path, \"AdaBoostClassifier\", \"abc_k5_r1_test.pkl\", folds=5, repeats=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sanity testing using other datasets\n",
    "\n",
    "Since the result is suspiciously good, we want to check if the code is faulty or not by testing it against other datasets and compare results. The main code that is tested here is train_cv_save_results(...). The data_preprocessing is not really tested here because well we're using other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The JIT palomba dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "def calc_vif_jit(X):\n",
    "\n",
    "    # Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    return(vif)\n",
    "\n",
    "def feature_selection_jit(data, colX, subset):\n",
    "    \n",
    "    X = data[colX]\n",
    "    vif1 = calc_vif_jit(X)\n",
    "    a=vif1.VIF.max()\n",
    "    while a > 5:\n",
    "        maximum_a = vif1.loc[vif1['VIF'] == vif1['VIF'].max()]\n",
    "        vif1 = vif1.loc[vif1['variables'] != maximum_a.iloc[0,0]]\n",
    "        vif1 = calc_vif_jit(X[vif1.variables.tolist()])\n",
    "        a = vif1.VIF.max()\n",
    "        # print(a)\n",
    "\n",
    "    X = data[vif1.variables.tolist()]\n",
    "    \n",
    "    return X\n",
    "\n",
    "def jit_data_preprocessing(subset, oversampling):\n",
    "    # Read in data_raw and create the variable df to manipulate it\n",
    "    df = pd.read_csv(jit_data, low_memory=False)\n",
    "\n",
    "    # remove infinite values and NaN values\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    # variables assignement\n",
    "    y = df['contributing']\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    y = lb.fit_transform(y).ravel()\n",
    "    if subset == 'CK':\n",
    "        colX = [c for c in feature_selection_jit(df, [c for c in df.columns if \"CK_\" in c], subset)]\n",
    "    if subset == 'PROCESS':\n",
    "        colX = [c for c in feature_selection_jit(df, [c for c in df.columns if \"P_\" in c], subset)]\n",
    "    if subset == 'TEXT':\n",
    "        colX = [c for c in df.columns if \"T_\" in c]\n",
    "    if subset == 'PROCESS-TEXT':\n",
    "        colX = [c for c in feature_selection_jit(df, [c for c in df.columns if \"P_\" in c], subset)] + [c for c in df.columns if \"T_\" in c]\n",
    "    if subset == 'CK-TEXT':\n",
    "        colX = [c for c in feature_selection_jit(df, [c for c in df.columns if \"CK_\" in c], subset)] + [c for c in df.columns if \"T_\" in c]\n",
    "    if subset == 'CK-PROCESS':\n",
    "        df['PROCESS_fix'] = df['PROCESS_fix'].astype(int)\n",
    "        colX = [c for c in feature_selection_jit(df, [c for c in df.columns if \"PRODUCT_\" in c or \"PROCESS_\" in c], subset)]\n",
    "        \n",
    "    if subset == 'ALL':\n",
    "        colX = [c for c in feature_selection_jit(df, [c for c in df.columns if \"CK_\" in c or \"P_\" in c], subset)] + [c for c in df.columns if \"T_\" in c]\n",
    "\n",
    "    groups = np.array(df['project'])\n",
    "\n",
    "    projects = np.unique(groups)\n",
    "\n",
    "    projects.sort()\n",
    "    j = 1\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "    group_list = []\n",
    "    for i in projects:\n",
    "        a = df[df.project.isin([i])]\n",
    "        a = a.sort_index(axis=0, inplace=False)\n",
    "        a_scaled = a[colX]\n",
    "        y = a['contributing']\n",
    "        lb = preprocessing.LabelBinarizer()\n",
    "        y = lb.fit_transform(y).ravel()\n",
    "\n",
    "\n",
    "        if oversampling == True:\n",
    "            if Counter(y)[1] > 1:\n",
    "                sm = SMOTE(random_state=0, n_jobs=-1, k_neighbors=1)\n",
    "                a_scaled, y = sm.fit_resample(a_scaled, y)\n",
    "\n",
    "\n",
    "        group_list.append([i]*len(y))\n",
    "        target_list.append(y)\n",
    "        input_list.append(a_scaled)\n",
    "        j += 1\n",
    "\n",
    "    X = np.concatenate(input_list)\n",
    "    y = np.concatenate(target_list)\n",
    "    groups = np.concatenate(group_list)\n",
    "    return X, y, groups, colX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jit = pd.read_csv(jit_data, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_jit.head\n",
    "print(\"JIT total length\", len(df_jit))\n",
    "print(\"JIT vuln contributing count\", len(df_jit[df_jit[\"contributing\"] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_jit, y_jit, groups_jit, colX_jit = jit_data_preprocessing('CK-PROCESS', True)\n",
    "print(X_jit.shape)\n",
    "print(y_jit.shape)\n",
    "print(groups_jit.shape)\n",
    "print('Using: %s' %colX_jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_jit_osf, y_jit_osf, groups_jit_osf, colX_jit_osf = jit_data_preprocessing('CK-PROCESS', False) #osf means oversampling false\n",
    "print(X_jit.shape)\n",
    "print(y_jit.shape)\n",
    "print(groups_jit.shape)\n",
    "print('Using: %s' %colX_jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_label_occurrences(labels):\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    label_counts = dict(zip(unique_labels, counts))\n",
    "    return label_counts\n",
    "\n",
    "count_label_occurrences(y_jit_osf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_jit = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\n",
    "train_cv_save_results(\"AdaBoostClassifier\", cv_jit, X_jit_osf, y_jit_osf, \"jit_process_product_k10_r1_oversamp_false.pkl\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_show_metrics(\"jit_process_product_k10_r1_oversamp_true.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_show_metrics(\"jit_process_product_k10_r1_oversamp_false.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_ratio = 1 \n",
    "rus = RandomUnderSampler(sampling_strategy=desired_ratio, random_state=42)\n",
    "X_jit_osf_bal, y_jit_osf_bal = rus.fit_resample(X_jit_osf, y_jit_osf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_jit_osf_bal.shape, y_jit_osf_bal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_jit = RepeatedStratifiedKFold(n_splits=9, n_repeats=1, random_state=1)\n",
    "train_cv_save_results(\"AdaBoostClassifier\", cv_jit, X_jit_osf_bal, y_jit_osf_bal, \"jit_process_product_k10_r1_oversamp_false_balanced.pkl\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_show_metrics(\"jit_process_product_k10_r1_oversamp_false_balanced.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MTML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
